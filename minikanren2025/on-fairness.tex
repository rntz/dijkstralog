\documentclass[acmsmall,screen,review,anonymous,dvipsnames,svgnames]{acmart}
\usepackage[utf8]{inputenc}

%\title{Monotonic, Bounded, Fair}
%% GEB: EGB reference how?
%\subtitle{miniKanren's power as inspiration; its limits as implementation}
%\subtitle{The power of miniKanren as inspiration; the limitations of miniKanren as implementation}

%\title{Worst-case optimal joins as fair conjunctions}
%\title{Fair, bounded, worst-case optimal joins}
\title{Fair intersection of seekable iterators}

\author{Michael Arntzenius}
\email{rntz@berkeley.edu}
\orcid{0009-0002-2234-2549}
\affiliation{%
	\institution{UC Berkeley}
	\city{Berkeley}
	\state{CA}
	\country{USA}
}

\bibliographystyle{ACM-Reference-Format}
\citestyle{acmauthoryear}


%% ACM keywords & CCS concepts
\keywords{miniKanren, seekable iterators, fair intersection, fair conjunction, worst-case optimal joins}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003752.10010070.10010111.10010113</concept_id>
       <concept_desc>Theory of computation~Database query languages (principles)</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10003752.10010070.10010111.10011711</concept_id>
       <concept_desc>Theory of computation~Database query processing and optimization (theory)</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10003752.10010070.10010111.10011734</concept_id>
       <concept_desc>Theory of computation~Logic and databases</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10003752.10010070.10010111.10011710</concept_id>
       <concept_desc>Theory of computation~Data structures and algorithms for data management</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10003752.10003809.10011254</concept_id>
       <concept_desc>Theory of computation~Algorithm design techniques</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10003752.10003809.10010031.10010033</concept_id>
       <concept_desc>Theory of computation~Sorting and searching</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10003752.10010124.10010125.10010127</concept_id>
       <concept_desc>Theory of computation~Functional constructs</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010148.10010149.10010157</concept_id>
       <concept_desc>Computing methodologies~Equation and inequality solving algorithms</concept_desc>
       <concept_significance>100</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Theory of computation~Database query languages (principles)}
\ccsdesc[500]{Theory of computation~Database query processing and optimization (theory)}
\ccsdesc[500]{Theory of computation~Logic and databases}
\ccsdesc[300]{Theory of computation~Data structures and algorithms for data management}
\ccsdesc[300]{Theory of computation~Algorithm design techniques}
\ccsdesc[300]{Theory of computation~Sorting and searching}
\ccsdesc[500]{Theory of computation~Functional constructs}
\ccsdesc[100]{Computing methodologies~Equation and inequality solving algorithms}


%% Packages & commands
\usepackage{minted}
\setminted{
  xleftmargin=1.5em,
  listparameters=\setlength{\topsep}{0.5em},
}

\newcommand\hask[1]{\mintinline{haskell}{#1}}
\newcommand\ttt\texttt

\newcommand\todo[1]{{\color{Orange}#1}}
\renewcommand\todo[1]{{\color{IndianRed}#1}}
%\renewcommand\todo[1]{{\color{DarkOrange}#1}}
%\renewcommand\todo[1]{{\color{OrangeRed}#1}}
%\renewcommand\todo[1]{{\color{VioletRed}#1}}
\newcommand\XXX{\todo{XXX}}


\begin{document}

\begin{abstract}
  {miniKanren}'s key semantic advance over Prolog is to implement a complete yet efficient search strategy, fairly interleaving execution between disjuncts.
  This fairness is accomplished by bounding how much work is done exploring one disjunct before switching to the next.
  We show that the same idea -- fairness via bounded work -- underlies an elegant compositional approach to implementing worst-case optimal joins using a seekable iterator interface, suitable for shallow embedding in functional languages.
\end{abstract}

\maketitle

\section{Fair union of streams}
%\section{Fair, bounded disjunction}

If there are multiple rules applicable to the current goal, Prolog tries them in order, exploring each to exhaustion before starting the next.
If exploring a rule fails to terminate, later rules are not visited, and any potential solutions they could have generated are lost.
%We might blame this on Prolog doing \emph{depth-first} search, which can get ``trapped'' exploring an infinite subtree and fail to return to explore its siblings.
If we think of goal-directed search as yielding a stream of solutions, Prolog implements disjunction (between multiple rules) as stream concatenation.
In Haskell, taking advantage of laziness to represent possibly-infinite streams, we might implement this like so:

\begin{minted}{haskell}
data Stream a = Empty
              | Yield a (Stream a)
append Empty        ys = ys
append (Yield x xs) ys = Yield x (append xs ys)         -- keep focus on xs
\end{minted}

\noindent
As we've hinted, concatenation is an incomplete search strategy: \ttt{append xs ys} will never visit \ttt{ys} if \ttt{xs} is infinite.
We can fix this by interleaving \ttt{xs} and \ttt{ys} instead:

\begin{minted}{haskell}
interleave Empty        ys = ys
interleave (Yield x xs) ys = Yield x (interleave ys xs) -- swap focus to ys
\end{minted}

\noindent
However, this is complete only if both streams are \emph{productive,} meaning that evaluation to weak head normal form always yields a constructor (\hask{Empty} or \hask{Yield}) without looping indefinitely.
It's easy to define unproductive streams recursively.
For instance, the logic program:

\begin{minted}{Prolog}
theAnswer(X) :- theAnswer(X).
theAnswer(42).
\end{minted}

\noindent
corresponds, if we implement disjunction via \ttt{interleave}, to an unproductive recursive stream:

\begin{minted}{haskell}
theAnswer :: Stream Int
theAnswer = interleave theAnswer (Yield 42 Empty)
\end{minted}

%% TODO: explain semantics, xs = xs ∪ {42}. Usual semantics is xs is the least set satisfying this, so xs = {42}; but *whatever* xs is, we definitely have 42 ∈ xs since 42 ∈ {42} ⇒ 42 ∈ xs ∪ {42} ⇒ 42 ∈ xs. But our approach here won't discover that; so it's not complete.

\noindent
There is no computability-theoretic reason for this unproductivity: we can search disjuncts in parallel, so in principle, the union of streams should be productive if \emph{any} input stream is.
However, neither concatenation nor interleaving has this property, and in fact, standard programming language evaluation orders (whether lazy or eager) cannot fairly interleave evaluation between two sub-expressions.
We must pick one to examine first; if it is unproductive, further evaluation is blocked.%
\footnote{Another classic computable-yet-undefinable function is ``parallel or'' \hask{por :: Bool -> Bool -> Bool} implementing fair boolean disjunction, meaning \hask{(por True undefined) == (por undefined True) == True}.}
%
The standard solution, used in e.g.~\textmu{}Kanren~\citep{muKanren}, is to extend \hask{Stream} with an extra constructor, \hask{Later},\footnotemark\ which exposes the existence of intermediate evaluation steps without giving any further information:

\begin{minted}{haskell}
data Stream a = Empty
              | Yield a (Stream a)
              | Later   (Stream a)
\end{minted}

\noindent
This makes it easy to ensure \emph{all} definitions are productive: simply guard any recursion with \hask{Later}.
\todo{TODO: cite guarded (co-)recursion.}
For instance, if we replace the canonical unproductive recursive definition, \hask{let xs = xs}, with a guarded one, \hask{let xs = Later xs}, examining \ttt{xs} will no longer cause an unrecoverable infinite loop; instead it will immediately yield \hask{Later xs}.
Thus \hask{Later} acts as an explicit signal to the consumer, permitting it to switch to evaluating some other stream if desired.
We can use this to implement fair stream union (\textmu{}Kanren's \ttt{mplus}):

\footnotetext{In \citet{muKanren} these are ``immature'' streams, represented as procedures of zero arguments.}

\begin{minted}{haskell}
union Empty        ys = ys
union (Yield x xs) ys = Yield x (union xs ys)  -- keep focus on xs
union (Later xs)   ys = Later   (union ys xs)  -- swap focus to ys
\end{minted}

\noindent
We can now define \ttt{theAnswer} completely, using \hask{Later} to guard the recursion:

\begin{minted}{haskell}
theAnswer :: Stream Int
theAnswer = union (Later theAnswer) (Yield 42 Empty)
-- Equivalent to infinitely interleaving ‘Later’ and ‘Yield 42’:
-- theAnswer = Later (Yield 42 theAnswer)
\end{minted}

\noindent
\todo{%
  TODO: explain this in terms of \emph{bounded work} instead of \emph{productivity}.
  (For instance, in the absence of any \hask{Later}s, this degenerates to concatenation; we are relying on \hask{Later} for completeness even if all streams are productive.)
  Explain that by guarding with \hask{Later}, we ensure a bounded amount of work is done before we hit a \hask{Later}.
  This means that switching only when we see \hask{Later} is fair, and therefore complete.
  Our design recipe, then, is to carefully \emph{bound} the work we do in any given search step, allowing us to \emph{fairly interleave} search steps.
  In minikanren this is used to ensure \emph{completeness.}
  As we're about to see, it can also be used to ensure \emph{performance.}
}

%% X and Y
%% --> evaluate X, it becomes (X1 : Xrest)
%% --> Y[X1] or (Xrest and Y)


\section{Unfair intersection of seekable iterators}

We will now show how to apply a similar trick, but for \emph{conjunctive} queries rather than \emph{disjunctive} ones -- in particular, database joins.
We will take the implementation of \emph{leapfrog intersection of seekable iterators} used in the worst-case optimal join algorithm Leapfrog Triejoin~\citep{lftj}, and show that it is \emph{not} fair.
To remedy this, we show how to extend the seekable iterator interface to allow \emph{bounded interleaving} between sub-iterators.

\todo{Explain that intersection of sorted sets is both a special case of, and an ingredient in the implementation of, database joins.}

Let us assume our data is sorted and allows efficiently seeking forward to find the next key-value pair whose key is at least some lower bound (by e.g.\ galloping search).
We can capture these assumptions in a \emph{seekable iterator} interface:

\begin{minted}{haskell}
data Iter k v = Empty
              | Yield k v (k -> Iter k v)
\end{minted}

\noindent
A seekable iterator, \hask{Iter k v}, is like a stream of key-value pairs, \hask{Stream (k,v)}, except that (a) it yields pairs in ascending key order, and (b) rather than the \emph{entire} remainder of the stream, \hask{Yield} produces a function \hask{seek :: k -> Iter k v} which seeks forward in it, i.e.\ \ttt{seek target} iterates over the remaining pairs with keys \ttt{k >= target}.
%
To recover the entire remainder, we simply pass \ttt{seek} the just-visited key; this lets us iterate over all elements of the stream:

\begin{minted}{haskell}
toSorted :: Iter k v -> [(k, v)]
toSorted Empty = []
toSorted (Yield k v seek) = (k,v) : toSorted (seek k)
\end{minted}

\noindent
Given a sorted list \hask{[(k, v)]}, we can easily produce a seekable iterator for it, although seeking will not be efficient since Haskell lists allow only linear, in-order access.
We could use a more appropriate data structure, such as a sorted array or balanced tree, but omit this as it is not crucial to our explanation:

\begin{minted}{haskell}
fromSorted :: Ord k => [(k, v)] -> Iter k v
fromSorted [] = Empty
fromSorted ((k,v) : rest) = Yield k v seek
  where seek k' = fromSorted (dropWhile ((< k') . fst) rest)
\end{minted}

\noindent
Finally, we can intersect two seekable iterators by leapfrogging: repeatedly advance the iterator at the smaller key towards the higher, until either both iterators reach the same key or one is exhausted:

\begin{minted}{haskell}
intersect :: Ord k => Iter k a -> Iter k b -> Iter k (a,b)
intersect Empty _ = Empty
intersect _ Empty = Empty
intersect s@(Yield k1 x s') t@(Yield k2 y t') =
  case compare k1 k2 of
    LT -> intersect (s' k2) t -- s < t, so seek s toward t
    GT -> intersect s (t' k1) -- t < s, so seek t toward s
    EQ -> Yield k1 (x,y) (\k' -> intersect (s' k') (t' k'))
\end{minted}

\noindent
%So far, so good.
However, \ttt{intersect}'s performance can suffer asymptotically when intersecting more than two iterators.
For instance, consider three subsets of the integers between 0 and 7,777,777 --- the evens, the odds, and the endpoints:

\begin{minted}{haskell}
evens = fromSorted [(x, "even") | x <- [0, 2 .. 7_777_777]]
odds  = fromSorted [(x, "odd")  | x <- [1, 3 .. 7_777_777]]
ends  = fromSorted [(x, "end")  | x <- [0,      7_777_777]]
\end{minted}

\noindent
The intersection of \ttt{evens} and \ttt{odds}, and therefore of all three sets, is empty.
We can compute this by calling \ttt{intersect} twice, but performance improves dramatically if, rather than intersecting \ttt{evens} and \ttt{odds} first, we intersect one of them with \ttt{ends} first.
At the GHCi repl:

\begin{minted}{haskell}
ghci> -- set +s to print time statistics
ghci> :set +s
ghci> toSorted ((evens `intersect` odds) `intersect` ends)
[]
(5.57 secs, 5,288,958,128 bytes)
ghci> toSorted (evens `intersect` (odds `intersect` ends))
[]
(0.57 secs, 248,961,040 bytes)
\end{minted}

\noindent
The reason is simple: ``leapfrogging'' \ttt{evens} and \ttt{odds} against one another performs a full linear scan of both lists.
Whatever our current key $x$ in \ttt{even} (e.g.\ $x = 1$), we seek forward past $x$ in \ttt{odds} and reach $x+1$; then we seek past $x+1$ in \ttt{even} to $x + 2$, and so on.
We do 7,777,777 units of work before we determine the intersection is empty.
%
By contrast, intersecting \ttt{odds} with \ttt{ends} almost immediately skips to the end of both relations.
(This occurs even though we are \emph{not} materializing any intermediate results.)%
\footnotemark{}

\footnotetext{
  We are telling a white lie here: because we use Haskell lists, seeking is linear-time, and there is no asymptotic slow-down.
  We are instead observing the difference between an \emph{interpreted} inner loop (\ttt{intersect}, loaded at the GHCi repl) and a \emph{compiled} one (\ttt{dropWhile} from the standard library).
  However, had we used sorted arrays with binary or galloping search, or balanced trees, there would be a true asymptotic speed-up for the reasons we describe.
}

The problem is that \ttt{intersect} does not (and with our current \hask{Iter} interface, \emph{cannot}) fairly interleave work between its arguments.
Instead, like \ttt{interleave}, it blocks first on one, then the other.
For this reason leapfrog intersection is usually formulated as an $n$-way operation (e.g.\ see \citet{lftj}).
Our binary \ttt{intersect} fails to composably capture the essence of this algorithm, which is \emph{to propagate lower bounds on keys between all intersected iterators.}
%When intersecting two iterators produced by \ttt{fromSorted}, we propagate information back and forth between them: the key we find in the first becomes our target in the second, and vice versa.
Evaluating \ttt{(evens `intersect` odds) `intersect` ends} immediately waits for \ttt{evens `intersect` odds} to find a key, which takes $O(n)$ work (where $n$ is the size of \ttt{evens}/\ttt{odds}).
This blocks information exchange between \ttt{ends} and \ttt{evens}/\ttt{odds} that would let us jump straight to the end in $O(1)$ time.

However, all is not lost, nor do we need to move to an $n$-way intersection primitive.
As with stream union, we can overcome this limitation by changing our interface and \emph{bounding} how much work we do.


\section{Fair intersection of seekable iterators}
%\section{Worst-case optimal iteration as bounded, fair conjunction}

Just as we implemented fair interleaving of streams by allowing a stream \emph{not} to yield an element, we will implement fair interleaving of seekable iterators by allowing them \emph{not} to yield a key-value pair.
Applying this lesson na\"ively, we might come up with:

\begin{minted}{haskell}
data Iter' k v = Empty
               | Yield k v (k -> Iter' k v)
               | Later     (k -> Iter' k v)
\end{minted}

\noindent
However, this is insufficiently expressive.
The essence of leapfrog intersection is to communicate lower bounds between intersected iterators.
\hask{Iter'} produces lower bounds only by yielding key-value pairs.
Yet if we interrupt a leapfrogging intersection while it is still searching for the next key-value pair, it may still be able to contribute a new lower bound -- for instance, while intersecting \ttt{evens} and \ttt{odds}, after \ttt{evens} proposes \ttt{k = 0} and \ttt{odds} seeks forward to reach \ttt{k = 1}, but before we seek again in \ttt{evens}, we already know that \ttt{k >= 1}.
%
Therefore instead we regard each iterator as having a \hask{Position}, which may be either a key-value pair or a lower bound on future keys:

\begin{minted}{haskell}
data Position k v = Found k v | Bound (Bound k)
data Bound k = Atleast k | Greater k | Done deriving Eq
\end{minted}

\noindent
\hask{Found k v} means we've found a key-value pair \ttt{(k,v)}.
\hask{Bound (Atleast k)} means all future keys are \ttt{>= k}.
\hask{Bound Done} means the iterator is exhausted.
We will understand the purpose of \hask{Greater} shortly.

We can now define the type \hask{Seek} of seekable iterators supporting fair intersection, which possess both a position and a seek function:

\begin{minted}{haskell}
data Seek k v = Seek
  { posn :: Position k v          -- a key-value pair, or a bound
  , seek :: Bound k -> Seek k v } -- seeks forward toward a bound
\end{minted}

\noindent
It simplifies the definition of intersection if seeking is idempotent; thus we require \ttt{seek} to consider the remainder of the sequence \emph{including} the current key, rather than dropping it as we did in \hask{Iter}.
%Unlike in \hask{Iter}, we require this seek function to consider the remainder of the sequence \emph{including} the current key, rather than dropping it; thus repeatedly seeking to a given bound is idempotent, which simplifies the definition of intersection.
We must take this into account when defining \ttt{toSorted}:

\begin{minted}[linenos]{haskell}
toSorted :: Seek k v -> [(k,v)]
toSorted (Seek (Bound Done) _)   = []
toSorted (Seek (Found k v) seek) = (k,v) : toSorted (seek (Greater k))
toSorted (Seek (Bound k)   seek) = toSorted (seek k)
\end{minted}

\noindent
\todo{TODO: also need to explain the \hask{Bound k} case where we retry!}
When the iterator has \hask{Found} a pair \ttt{(k,v)}, we pass \hask{Greater k} to its seek function to advance \emph{beyond} the key \ttt{k}.
The idea is that \ttt{seek b} seeks towards the first (smallest) key satisfying the bound \ttt{b}.
We already know that \hask{Atleast lo} is satisfied by keys \ttt{k >= lo}.
\hask{Greater lo} is satisfied only by \emph{strictly greater} keys, \ttt{k > lo}.
And \hask{Done} is satisfied by no keys whatsoever.
This endows bounds with a natural order: for bounds \ttt{p,q} we let \ttt{p <= q} iff any key satisfying \ttt{q} must satisfy \ttt{p}.
We can implement this concisely, if nonobviously, as follows:

\begin{minted}{haskell}
instance Ord k => Ord (Bound k) where
  compare x y = embed x `compare` embed y where
    embed (Atleast k) = (1, Just (k, 1))
    embed (Greater k) = (1, Just (k, 2))
    embed Done        = (2, Nothing)

satisfies :: Ord k => Bound k -> k -> Bool
satisfies bound k = bound <= Atleast k
\end{minted}

\noindent
Using \ttt{satisfies} we can define \ttt{fromSorted}:

\begin{minted}{haskell}
fromSorted :: Ord k => [(k,v)] -> Seek k v
fromSorted l = Seek posn seek
  where posn = case l of (k,v):_ -> Found k v
                         []      -> Bound Done
        seek target = fromSorted (dropWhile (not . satisfies target . fst) l)
\end{minted}

\noindent
To define intersection we need one last helper, which extracts a lower bound on the remaining keys in an iterator:

\begin{minted}{haskell}
bound :: Seek k v -> Bound k
bound (Seek (Found k _) _) = Atleast k
bound (Seek (Bound p)   _) = p
\end{minted}

\noindent
Finally, we can define fair intersection of seekable iterators.
If both iterators are at the same key and have found values, we've found an element of the intersection; otherwise we only know there are no keys until after the greater of their bounds.
To seek, we simply seek our sub-iterators.
\todo{Point out that this \emph{bounds} the work done by a single call to seek.}

\begin{minted}{haskell}
intersect :: Ord k => Seek k a -> Seek k b -> Seek k (a,b)
intersect s t = Seek posn' seek' where
  posn' | Found k x <- posn s, Found k' y <- posn t, k == k' = Found k (x, y)
        | otherwise = Bound (bound s `max` bound t)
  seek' k = seek s k `intersect` seek t k
\end{minted}

\noindent
\todo{TODO: ok now what do I say? Oh, obviously I test this on my previous example: evens, odds, ends.
  UNFORTUNATELY, this (a) runs straight into the white lie I told earlier -- it's super slow unless we compile it; (b) when we compile, the three-way intersection is indeed faster than (intersect evens odds), which is good, but (c) why should it be? it's the same asymptotic complexity! it appears that dropWhile is ~3.5--4$\times$ faster than ping-ponging between evens \& odds, which is... a larger constant factor than I expected, but okay.
}


%% \begin{figure}
%%   \begin{minted}[xleftmargin=0em, linenos]{haskell}
%% data Bound k
%%   = Init             -- anything satisfies Init
%%   | Atleast k        -- is it >= k?
%%   | Greater k        -- is it  > k?
%%   | Done             -- nothing satisfies Done
%%   deriving Eq

%% data Position k v
%%   = Found k v        -- here's a key and its value
%%   | Bound (Bound k)  -- the next key satisfies this bound

%% data Seek k v = Seek
%%   { posn :: Position k v          -- a key-value pair, or a bound
%%   , seek :: Bound k -> Seek k v } -- seeks forward toward a bound

%% -- Init < ... < Atleast n < Greater n < Atleast (n+1) < ... < Done
%% instance Ord k => Ord (Bound k) where
%%   compare x y = embed x `compare` embed y
%%     where embed Init        = (0, Nothing)
%%           embed (Atleast k) = (1, Just (k, 0))
%%           embed (Greater k) = (1, Just (k, 1))
%%           embed Done        = (2, Nothing)

%% bound :: Seek k v -> Bound k
%% bound (Seek (Found k v) _) = Atleast k
%% bound (Seek (Bound p)   _) = p

%% fromSorted :: Ord k => [(k, v)] -> Seek k v
%% fromSorted l = Seek posn seek where
%%   posn = case l of [] -> Bound Done; (k,v):_ -> Found k v
%%   seek target = fromSorted $ dropWhile ((target >) . Atleast . fst) l

%% toSorted :: Seek k v -> [(k, v)]
%% toSorted (Seek (Bound Done) _)    = []
%% toSorted (Seek (Found k v)  seek) = (k, v) : toSorted (seek (Greater k))
%% toSorted (Seek (Bound k)    seek) = toSorted (seek k)

%% intersect :: Ord k => Seek k a -> Seek k b -> Seek k (a,b)
%% intersect s t = Seek posn' seek' where
%%   posn' | Found k x <- posn s, Found k' y <- posn t, k == k' = Found k (x, y)
%%         | otherwise = Bound (bound s `max` bound t)
%%   seek' k = intersect s' t' where
%%     s' = seek1 k
%%     t' = seek2 (bound s') -- leapfrog optimization; could be (seek2 k) instead
%%   \end{minted}
%%   \caption{Fair intersection of seekable iterators}
%%   \label{fig:fair-iterators}
%% \end{figure}


%% \section{sketches}
%% \todo{TODO SKETCHES}

%% miniKanren's key idea over Prolog is to have a complete search strategy by using a FAIR strategy for disjunctions.
%% We accomplish this by BOUNDING the time we spend in each disjunct before switching to the other.

%% FAIR: no branch gets ``starved'' by another branch; with enough time, we investigate all branches arbitrarily far.

%% BOUNDED: we try a branch for only a bounded amount of time, so that we don't get STUCK. This is the purpose of the ``thunk'' constructor for streams (what's the standard name for this in mK?).

%% Two uses of fairness:

%% 1. Implementing $\lambda_{\vee}$.
%%    semantics are nondeterministic,
%%    so SEARCH!

%%    This search is inefficient because it fails to take into account the *monotonic structure* of evaluation in lambda join: we are in essence branching on *how precise* to make our evaluation.
%%    The branching is not between mutually exclusive alternatives, where each contributes information the other lacks, but where one alternative has strictly more information than another.

%%    (how does this actually result in combinatorial explosion, though?)

%% 2. Work in progress on a seekable iterator interface for compositionally worst-case optimal joins.
%%    The key idea is to FAIRLY incorporate information from all iterators contributing to the join,
%%    which we do by BOUNDING how much work we put into one iterator before moving to the next.

%%    This implements FAIR CONJUNCTION, and it's FAST.

%%    But it takes advantage of having a comprehensive view of data, so we can scan through it in sorted order - we can't define these things recursively, therefore it can't be turing-complete.
%% if we decide to use ``feedback with delay'' to overcome this, we have perhaps reinvented, not miniKanren, but Datalog!


\section{Related and future work}

\todo{by repeatedly intersecting on different variables (trie style), we get worst-case optimal joins!}

\todo{this is a form of fair conjunction.}

\todo{this can't handle arbitrary recursion; we'd need to introduce a time delay element. (Curiously, this might bring us closer to Datalog, which finds fixed points by iteration through an arbitrary time dimension!)}

\todo{indexed streams reference}


%% ---------- BIBLIOGRAPHY ----------
\bibliography{on-fairness}

\end{document}

\documentclass[acmsmall,screen,review,anonymous,dvipsnames,svgnames]{acmart}
\usepackage[utf8]{inputenc}

%\title{Monotonic, Bounded, Fair}
%% GEB: EGB reference how?
%\subtitle{miniKanren's power as inspiration; its limits as implementation}
%\subtitle{The power of miniKanren as inspiration; the limitations of miniKanren as implementation}

%\title{Worst-case optimal joins as fair conjunctions}
%\title{Fair, bounded, worst-case optimal joins}
\title{Fair intersection of seekable iterators}

\author{Michael Arntzenius}
\email{rntz@berkeley.edu}
\orcid{0009-0002-2234-2549}
\affiliation{%
	\institution{UC Berkeley}
	\city{Berkeley}
	\state{CA}
	\country{USA}
}

\bibliographystyle{ACM-Reference-Format}
\citestyle{acmauthoryear}


%% %% ACM keywords & CCS concepts
%% \keywords{miniKanren, seekable iterators, fair intersection, fair conjunction, worst-case optimal joins}

%% \begin{CCSXML}
%% <ccs2012>
%%    <concept>
%%        <concept_id>10003752.10010070.10010111.10010113</concept_id>
%%        <concept_desc>Theory of computation~Database query languages (principles)</concept_desc>
%%        <concept_significance>500</concept_significance>
%%        </concept>
%%    <concept>
%%        <concept_id>10003752.10010070.10010111.10011711</concept_id>
%%        <concept_desc>Theory of computation~Database query processing and optimization (theory)</concept_desc>
%%        <concept_significance>500</concept_significance>
%%        </concept>
%%    <concept>
%%        <concept_id>10003752.10010070.10010111.10011734</concept_id>
%%        <concept_desc>Theory of computation~Logic and databases</concept_desc>
%%        <concept_significance>500</concept_significance>
%%        </concept>
%%    <concept>
%%        <concept_id>10003752.10010070.10010111.10011710</concept_id>
%%        <concept_desc>Theory of computation~Data structures and algorithms for data management</concept_desc>
%%        <concept_significance>300</concept_significance>
%%        </concept>
%%    <concept>
%%        <concept_id>10003752.10003809.10011254</concept_id>
%%        <concept_desc>Theory of computation~Algorithm design techniques</concept_desc>
%%        <concept_significance>300</concept_significance>
%%        </concept>
%%    <concept>
%%        <concept_id>10003752.10003809.10010031.10010033</concept_id>
%%        <concept_desc>Theory of computation~Sorting and searching</concept_desc>
%%        <concept_significance>300</concept_significance>
%%        </concept>
%%    <concept>
%%        <concept_id>10003752.10010124.10010125.10010127</concept_id>
%%        <concept_desc>Theory of computation~Functional constructs</concept_desc>
%%        <concept_significance>500</concept_significance>
%%        </concept>
%%    <concept>
%%        <concept_id>10010147.10010148.10010149.10010157</concept_id>
%%        <concept_desc>Computing methodologies~Equation and inequality solving algorithms</concept_desc>
%%        <concept_significance>100</concept_significance>
%%        </concept>
%%  </ccs2012>
%% \end{CCSXML}

%% \ccsdesc[500]{Theory of computation~Database query languages (principles)}
%% \ccsdesc[500]{Theory of computation~Database query processing and optimization (theory)}
%% \ccsdesc[500]{Theory of computation~Logic and databases}
%% \ccsdesc[300]{Theory of computation~Data structures and algorithms for data management}
%% \ccsdesc[300]{Theory of computation~Algorithm design techniques}
%% \ccsdesc[300]{Theory of computation~Sorting and searching}
%% \ccsdesc[500]{Theory of computation~Functional constructs}
%% \ccsdesc[100]{Computing methodologies~Equation and inequality solving algorithms}


%% Packages & commands
\usepackage{minted}
\setminted{
  xleftmargin=1.5em,
  listparameters=\setlength{\topsep}{0.5em},
}

\usepackage[nameinlink,noabbrev]{cleveref}

\newcommand\hask[1]{\mintinline{haskell}{#1}}
\newcommand\ttt\texttt

\newcommand\todo[1]{{\color{Orange}#1}}
\renewcommand\todo[1]{{\color{IndianRed}#1}}
%\renewcommand\todo[1]{{\color{DarkOrange}#1}}
%\renewcommand\todo[1]{{\color{OrangeRed}#1}}
%\renewcommand\todo[1]{{\color{VioletRed}#1}}
\newcommand\XXX{\todo{XXX}}

\renewcommand\todo[1]{\ignorespaces}


\begin{document}

\begin{abstract}
  miniKanren's key semantic advance over Prolog is to implement a complete yet efficient search strategy, fairly interleaving execution between disjuncts.
  This fairness is accomplished by bounding how much work is done exploring one disjunct before switching to the next.
  We show that the same idea---fairness via bounded work---underlies an elegant compositional approach to implementing worst-case optimal joins using a seekable iterator interface, suitable for shallow embedding in functional languages.
\end{abstract}

\maketitle

\section{Fair union of streams}
%\section{Fair, bounded disjunction}

When multiple rules apply to the current goal, Prolog tries them in order, exploring each to exhaustion before starting the next.
If exploring a rule fails to terminate, later rules are not visited, and potential solutions they might generate are lost.
If we think of goal-directed search as yielding a stream of solutions, Prolog implements disjunction between rules as stream concatenation.
In Haskell, using laziness to represent possibly-infinite streams, we might implement this like so:

\begin{minted}{haskell}
data Stream a = Empty
              | Yield a (Stream a)
append Empty        ys = ys
append (Yield x xs) ys = Yield x (append xs ys)         -- keep focus on xs
\end{minted}

\noindent
As we've hinted, concatenation is an incomplete search strategy: \ttt{append xs ys} will never visit \ttt{ys} if \ttt{xs} is infinite.
We can fix this by interleaving \ttt{xs} and \ttt{ys} instead:

\begin{minted}{haskell}
interleave Empty        ys = ys
interleave (Yield x xs) ys = Yield x (interleave ys xs) -- swap focus to ys
\end{minted}

\noindent
However, this is complete only if both streams are \emph{productive}---they must not loop indefinitely without producing either \hask{Empty} or \hask{Yield}.
It's easy to define unproductive streams recursively.
For instance, the logic program:

\begin{minted}{Prolog}
theAnswer(X) :- theAnswer(X).
theAnswer(42).
\end{minted}

\noindent
corresponds, if we implement disjunction via \ttt{interleave}, to an unproductive recursive stream:

\begin{minted}{haskell}
theAnswer :: Stream Int
theAnswer = interleave theAnswer (Yield 42 Empty)
\end{minted}

%% TODO: explain semantics, xs = xs ∪ {42}. Usual semantics is xs is the least set satisfying this, so xs = {42}; but *whatever* xs is, we definitely have 42 ∈ xs since 42 ∈ {42} ⇒ 42 ∈ xs ∪ {42} ⇒ 42 ∈ xs. But our approach here won't discover that; so it's not complete.

\noindent
From a computability perspective, this unproductivity is unnecessary: if we search in parallel, the union of streams can yield if at least one of its input streams can.
%There is no computability-theoretic reason for this unproductivity: we can search disjuncts in parallel, so the union of streams could in principle be productive if \emph{any} input stream is.
However, neither concatenation nor interleaving has this property, and in fact, standard programming language evaluation orders (lazy or eager) cannot fairly interleave evaluation between two sub-expressions.
We must pick one to examine first, and if it is unproductive, further evaluation is blocked.%
\footnote{Another classic computable-yet-undefinable function is ``parallel or'' \hask{por :: Bool -> Bool -> Bool} implementing fair boolean disjunction, meaning \hask{(por True undefined) == (por undefined True) == True}.}
%
One solution, used in e.g.\ \textmu{}Kanren~\citep{muKanren}, is to extend \hask{Stream} with a constructor \hask{Later} that exposes the existence of intermediate evaluation steps without giving any further information:\footnotemark

\footnotetext{In \citet{muKanren} these are ``immature'' streams, represented as procedures of zero arguments.}

\begin{minted}{haskell}
data Stream a = Empty
              | Yield a (Stream a)
              | Later   (Stream a)
\end{minted}

\noindent
%This makes it easy to ensure \emph{all} definitions are productive:
This allows any definition to be productive:
simply guard recursion with \hask{Later}.\footnote{
There is a long line of work on type systems that ensure productivity by checking recursion is appropriately guarded in this fashion~\citetext{e.g.\ \citealp{DBLP:conf/lics/Nakano00}, \citealp{DBLP:journals/corr/abs-1208-3596},  \citealp{DBLP:conf/icfp/AtkeyM13}}.
  Such type systems would have caught our ``mistake'' in defining \texttt{theAnswer} recursively without guarding it.
}
%
For instance, if we replace the unproductive \hask{let xs = xs} with the guarded \hask{let xs = Later xs}, examining \ttt{xs} no longer causes an unrecoverable infinite loop, instead immediately yielding \hask{Later xs}.
Thus \hask{Later} acts as an explicit signal to the consumer, permitting it to switch to evaluating something else.
%Thus \hask{Later} explicitly yields control to the consumer, permitting it to switch to evaluating something else.
%The purpose of \hask{Later} is to yield control to the consumer, so it may switch to evaluating something else.
This lets us implement fair stream union (\textmu{}Kanren's \ttt{mplus}):\footnotemark{}

\footnotetext{
%In fact, this is more than merely productive.
%We are actually using a slightly stronger property than productivity here.
Since \ttt{union} concatenates on \hask{Yield} and interleaves on \hask{Later}, and concatenation is an incomplete search strategy for infinite lists, \ttt{union} is complete only for streams which \hask{Yield} at most finitely often between \hask{Later}s.
Fortunately, guarding recursion with \hask{Later} also guarantees this property, which we might call ``strong productivity'':
%
productivity means we do a finite amount of work before we yield control to our caller;
%If we regard all constructors as yielding control, we get the usual meaning of productivity in lazy languages;
%In lazy languages, all constructors yield control;
strong productivity replaces ``yield control'' with ``yield \hask{Later}.''
%
For this reason, in an eager language, only \hask{Later}'s argument must be thunked, not \hask{Yield}'s.
  Likewise, in type theories for guarded recursion, only \hask{Later}'s argument must be guarded with a ``later'' modality.
%  Of course, it can be beneficial for performance to also delay evaluation of the arguments to \hask{Yield}, as typical miniKanren implementations do.
}

%% \footnotetext{
%% %In fact, this is more than merely productive.
%% %We are actually using a slightly stronger property than productivity here.
%% Observe that \ttt{union} is effectively ``concatenate on \hask{Yield}, interleave on \hask{Later}.''
%% Since concatenation is an incomplete search strategy for infinite lists, \ttt{union} is complete only for {``strongly productive''} streams, which have at most finitely many \hask{Yield}s between \hask{Later}s.
%% %
%% Fortunately, since infinity comes from recursion, guarding recursion with \hask{Later} also guarantees strong productivity.
%% %This is no harder to ensure than productivity (infinity comes only from recursion; so guard all recursion with \hask{Later}), but it is formally stronger.

%% Made precise, productivity means we do a finite amount of work before we yield control to our caller;
%% %If we regard all constructors as yielding control, we get the usual meaning of productivity in lazy languages;
%% %In lazy languages, all constructors yield control;
%% strong productivity strengthens ``yield control'' to ``yield \hask{Later}.''
%% %
%% For this reason, in an eager language, only \hask{Later}'s argument needs to be thunked, not \hask{Yield}'s.
%%   Likewise, in type theories for guarded recursion, it is \hask{Later}'s argument which must be guarded with a ``later'' modality.
%% %  Of course, it can be beneficial for performance to also delay evaluation of the arguments to \hask{Yield}, as typical miniKanren implementations do.
%% }

\begin{minted}{haskell}
union Empty        ys = ys
union (Yield x xs) ys = Yield x (union xs ys)  -- keep focus on xs
union (Later xs)   ys = Later   (union ys xs)  -- swap focus to ys
\end{minted}

\noindent
We can now define \ttt{theAnswer} completely, using \hask{Later} to guard the recursion:

\begin{minted}{haskell}
theAnswer :: Stream Int
theAnswer = union (Later theAnswer) (Yield 42 Empty)
-- Equivalent to endlessly interleaving ‘Later’ with ‘Yield 42’:
-- theAnswer = Later (Yield 42 theAnswer)
\end{minted}

%% \noindent
%% %In fact, this is more than merely productive.
%% %We are actually using a slightly stronger property than productivity here.
%% Observe that \ttt{union} is effectively ``concatenate on \hask{Yield}, interleave on \hask{Later}.''
%% Since concatenation is an incomplete search strategy for infinite lists, \ttt{union} is complete only for {``strongly productive''} streams with at most finitely many \hask{Yield}s between \hask{Later}s.
%% %This is no harder to ensure than productivity (infinity comes only from recursion; so guard all recursion with \hask{Later}), but it is formally stronger.
%% %
%% Productivity means {we do a finite amount of work before yielding control to our caller.}
%% %If we regard all constructors as yielding control, we get the usual meaning of productivity in lazy languages;
%% %In lazy languages, all constructors yield control;
%% Strong productivity strengthens ``yielding control'' to ``yielding \hask{Later}.''\footnotemark{}
%% %
%% Since infinity comes from recursion, guarding recursion with \hask{Later} also guarantees strong productivity.

%% \footnotetext{For this reason, in an eager language, only \hask{Later}'s argument needs to be thunked, not \hask{Yield}'s.
%%   Likewise, in type theories for guarded recursion, it is \hask{Later}'s argument which must be guarded with a ``later'' modality.
%% %  Of course, it can be beneficial for performance to also delay evaluation of the arguments to \hask{Yield}, as typical miniKanren implementations do.
%% }

\noindent
Fair disjunction, then, is accomplished by bounding the work a stream does before handing control to the next stream.
In the next two sections, we'll apply a similar trick to intersection rather than union.
In \cref{sec:unfair-intersection} we will show that the leapfrog intersection of seekable iterators used in the worst-case optimal join algorithm Leapfrog Triejoin~\citep{lftj} is \emph{not} fair.
To remedy this, in \cref{sec:fair-intersection} we show how to extend the seekable iterator interface to allow \emph{bounded interleaving} between sub-iterators. \todo{TODO: rephrase `bounded interleaving'?}


%% ---------- SCRAPS, TODO: REUSE ----------
%% \todo{
%%   In minikanren this is used to ensure \emph{completeness.}
%%   As we're about to see, it can also be used to ensure \emph{performance.}
%%   }

%% In the next two sections, we will further strengthen productivity from \emph{termination} to \emph{performance.}
%% Where termination is binary (finite or infinite work?), performance is a spectrum: how \emph{much} work?
%% To implement fair disjunction, we bound the work any stream does before yielding control, allowing us to fairly interleave multiple streams.
%% By applying the same trick we can implement fair \emph{conjunction}

%% In the next two sections, we will further strengthen productivity from \emph{termination} to \emph{performance.}
%% Where termination is binary (finite or infinite work?), performance is a spectrum: how \emph{much} work?
%% By \emph{bounding} how much work 

%% \todo{generalize ``finite'' to ``bounded in some way'' and get EFFICIENCY not PRODUCTIVITY.}

%% \todo{%
%%   TODO: explain this in terms of \emph{bounded work} instead of \emph{productivity}.
%%   %(For instance, in the absence of any \hask{Later}s, this degenerates to concatenation; we are relying on \hask{Later} for completeness even if all streams are productive.)
%%   %Explain that by guarding with \hask{Later}, we ensure a bounded amount of work is done before we hit a \hask{Later}.
%%   %This means that switching only when we see \hask{Later} is fair, and therefore complete.
%%   Our design recipe, then, is to carefully \emph{bound} the work we do in any given search step, allowing us to \emph{fairly interleave} search steps.
%%   In minikanren this is used to ensure \emph{completeness.}
%%   As we're about to see, it can also be used to ensure \emph{performance.}
%% }

%% \todo{Explain that intersection of sorted sets is both a special case of, and an ingredient in the implementation of, database joins.}

%% X and Y
%% --> evaluate X, it becomes (X1 : Xrest)
%% --> Y[X1] or (Xrest and Y)


\section{Unfair intersection of seekable iterators}
\label{sec:unfair-intersection}

Suppose we have a collection of key-value pairs, stored in sorted order so that we can efficiently seek forward to find the next pair whose key satisfies some lower bound (by e.g.\ galloping search).
We can capture these assumptions in a \emph{seekable iterator} interface:

\begin{minted}{haskell}
data Iter k v = Empty
              | Yield k v (k -> Iter k v)
\end{minted}

\noindent
A seekable iterator, \hask{Iter k v}, is like a stream of key-value pairs, \hask{Stream (k,v)}, except that (a) it yields pairs in ascending key order, and (b) rather than the entire remainder of the stream, \hask{Yield} produces a function \hask{seek :: k -> Iter k v}
%which seeks forward, i.e.\
such that
\ttt{seek target} iterates over the remaining pairs with keys \ttt{k >= target}.
%
To recover the entire remainder, we pass \ttt{seek} the just-visited key; this lets us iterate over all elements of the stream:

\begin{minted}{haskell}
toSorted :: Iter k v -> [(k, v)]
toSorted Empty = []
toSorted (Yield k v seek) = (k,v) : toSorted (seek k)
\end{minted}

\noindent
We can easily turn a sorted list \hask{[(k, v)]} into a seekable iterator, although seeking will not be efficient since Haskell lists allow only linear, in-order access.
We could use a more appropriate data structure, such as a sorted array or balanced tree, but omit this as it is not crucial to our explanation:

\begin{minted}{haskell}
fromSorted :: Ord k => [(k, v)] -> Iter k v
fromSorted [] = Empty
fromSorted ((k,v) : rest) = Yield k v seek
  where seek k' = fromSorted (dropWhile ((< k') . fst) rest)
\end{minted}

\noindent
We can intersect two seekable iterators by leapfrogging: repeatedly advance the iterator at the smaller key towards the higher, until either both iterators reach the same key or one is exhausted:

\begin{minted}{haskell}
intersect :: Ord k => Iter k a -> Iter k b -> Iter k (a,b)
intersect Empty _ = Empty
intersect _ Empty = Empty
intersect s@(Yield k1 x s') t@(Yield k2 y t') =
  case compare k1 k2 of
    LT -> intersect (s' k2) t -- s < t, so seek s toward t
    GT -> intersect s (t' k1) -- t < s, so seek t toward s
    EQ -> Yield k1 (x,y) (\k' -> intersect (s' k') (t' k'))
\end{minted}

\noindent
%So far, so good.
However, \ttt{intersect}'s performance can suffer asymptotically when intersecting more than two iterators.
For instance, consider three subsets of the integers between 0 and 7,777,777---the evens, the odds, and the endpoints:

\begin{minted}{haskell}
evens = fromSorted [(x, "even") | x <- [0, 2 .. 7_777_777]]
odds  = fromSorted [(x, "odd")  | x <- [1, 3 .. 7_777_777]]
ends  = fromSorted [(x, "end")  | x <- [0,      7_777_777]]
\end{minted}

\noindent
The intersection of \ttt{evens} and \ttt{odds}, and therefore of all three sets, is empty.
We can compute this by calling \ttt{intersect} twice, but performance improves dramatically if, rather than intersecting \ttt{evens} and \ttt{odds} first, we intersect one of them with \ttt{ends} first.
At the GHCi repl:

\begin{minted}{haskell}
ghci> -- set +s to print time statistics
ghci> :set +s
ghci> toSorted ((evens `intersect` odds) `intersect` ends)
[]
(5.57 secs, 5,288,958,128 bytes)
ghci> toSorted (evens `intersect` (odds `intersect` ends))
[]
(0.57 secs, 248,961,040 bytes)
\end{minted}

\noindent
The reason is simple: ``leapfrogging'' \ttt{evens} and \ttt{odds} against one another performs a full linear scan of both lists.
Whatever our current key $x$ in \ttt{even} (e.g.\ $x = 1$), we seek forward past $x$ in \ttt{odds} and reach $x+1$; then we seek past $x+1$ in \ttt{even} to $x + 2$, and so on.
We do 7,777,777 units of work before we determine the intersection is empty.
%
By contrast, intersecting \ttt{odds} with \ttt{ends} almost immediately skips to the end of both relations.
(This occurs even though we are \emph{not} materializing any intermediate results.)%
\footnote{\label{fn:white-lie}%
  We are telling a white lie here: because we use Haskell lists, seeking is linear-time, and there is no asymptotic slow-down.
  We are instead observing the difference between an \emph{interpreted} inner loop (\ttt{intersect}, loaded at the GHCi repl) and a \emph{compiled} one (\ttt{dropWhile} from the standard library).
  However, had we used sorted arrays with binary or galloping search, or balanced trees, there would be a true asymptotic speed-up for the reasons we describe.
}

The problem is that \ttt{intersect} does not (and with our \hask{Iter} interface, \emph{cannot}) fairly interleave work between its arguments.
Instead, like \ttt{interleave}, it blocks first on one, then the other.
For this reason leapfrog intersection is usually formulated as an $n$-way operation (e.g.\ see \citet{lftj}).
%, with the expectation that it will be applied ``flat'' to all intersected iterators, rather than ``nested''---the output of leapfrog intersection, despite presenting a seekable iterator interface, cannot be used as input to leapfrog intersection without risking performance loss.
Our binary \ttt{intersect} fails to composably capture the essence of this algorithm, which is \emph{to propagate lower bounds on keys between all intersected iterators.}
%When intersecting two iterators produced by \ttt{fromSorted}, we propagate information back and forth between them: the key we find in the first becomes our target in the second, and vice versa.
Evaluating \ttt{(evens `intersect` odds) `intersect` ends} immediately waits for \ttt{evens `intersect` odds} to find a key, which takes $O(n)$ work (where $n$ is the size of \ttt{evens}/\ttt{odds}).
This blocks information exchange between \ttt{ends} and \ttt{evens}/\ttt{odds} that would let us jump straight to the end in $O(1)$ time.

However, all is not lost, nor must we move to an $n$-way intersection primitive.
As with stream union, we can side-step the issue by changing our interface to \emph{bound} how much work we do.


\section{Fair intersection of seekable iterators}
%\section{Worst-case optimal iteration as bounded, fair conjunction}
\label{sec:fair-intersection}

Just as we implemented fair interleaving of streams by allowing a stream \emph{not} to yield an element, we will implement fair interleaving of seekable iterators by allowing them \emph{not} to yield a key-value pair.
Applying this lesson na\"ively, we might come up with:

\begin{minted}{haskell}
data Iter' k v = Empty
               | Yield k v (k -> Iter' k v)
               | Later     (k -> Iter' k v)
\end{minted}

\noindent
However, this is insufficiently expressive.
The essence of leapfrog intersection is to communicate lower bounds between intersected iterators.
\hask{Iter'} produces lower bounds only by yielding key-value pairs.
Yet if we interrupt a leapfrogging intersection while it is still searching for the next key-value pair, it may still be able to contribute a new lower bound---for instance, while intersecting \ttt{evens} and \ttt{odds}, after \ttt{evens} proposes \ttt{k = 0} and \ttt{odds} seeks forward to reach \ttt{k = 1}, but before we seek again in \ttt{evens}, we already know that \ttt{k >= 1}.
%
Therefore instead we regard each iterator as having a \hask{Position}, which may be either a key-value pair or a lower bound on future keys:

\begin{minted}{haskell}
data Position k v = Found k v | Bound (Bound k)
data Bound k = Atleast k | Greater k | Done deriving Eq
\end{minted}

\noindent
\hask{Found k v} means we've found a key-value pair \ttt{(k,v)}.
\hask{Bound (Atleast k)} means all future keys are \ttt{>= k}.
\hask{Bound Done} means the iterator is exhausted.
We will see the purpose of \hask{Greater} shortly.

We can now define the type \hask{Seek} of seekable iterators supporting fair intersection, which possess both a position and a seek function:

\begin{minted}{haskell}
data Seek k v = Seek
  { posn :: Position k v          -- a key-value pair, or a bound
  , seek :: Bound k -> Seek k v } -- seeks forward toward a bound
\end{minted}

\noindent
It simplifies the definition of intersection if seeking is idempotent; thus we require \ttt{seek} to consider the remainder of the sequence \emph{including} the current key, rather than dropping it as we did in \hask{Iter}.
%Unlike in \hask{Iter}, we require this seek function to consider the remainder of the sequence \emph{including} the current key, rather than dropping it; thus repeatedly seeking to a given bound is idempotent, which simplifies the definition of intersection.
We must take this into account when defining \ttt{toSorted}:

\begin{minted}{haskell}
toSorted :: Seek k v -> [(k,v)]
toSorted (Seek (Bound Done) _)    = []
toSorted (Seek (Bound p)    seek) = toSorted (seek p)
toSorted (Seek (Found k v)  seek) = (k,v) : toSorted (seek (Greater k))
\end{minted}

\noindent
\todo{TODO: also need to explain the \hask{Bound k} case where we retry!}
When the iterator has \hask{Found} a pair \ttt{(k,v)}, we pass \hask{Greater k} to its seek function to advance \emph{beyond} the key \ttt{k}.
The idea is that \ttt{seek b} seeks towards the first (smallest) key satisfying the bound \ttt{b}.
We already know that \hask{Atleast lo} is satisfied by keys \ttt{k >= lo}.
\hask{Greater lo} is satisfied only by \emph{strictly greater} keys, \ttt{k > lo}.
And \hask{Done} is satisfied by no keys whatsoever.
This endows bounds with a natural order: for bounds \ttt{p,q} we let \ttt{p <= q} iff any key satisfying \ttt{q} must satisfy \ttt{p}.
We can implement this concisely, if nonobviously, as follows:

\begin{minted}{haskell}
satisfies :: Ord k => Bound k -> k -> Bool
satisfies bound k = bound <= Atleast k
instance Ord k => Ord (Bound k) where
  compare x y = embed x `compare` embed y where
    embed (Atleast k) = (1, Just (k, 1))
    embed (Greater k) = (1, Just (k, 2))
    embed Done        = (2, Nothing)
\end{minted}

\noindent
Using \ttt{satisfies} we can define \ttt{fromSorted}:

\begin{minted}{haskell}
fromSorted :: Ord k => [(k,v)] -> Seek k v
fromSorted l = Seek posn seek where
  posn = case l of (k,v):_ -> Found k v
                   []      -> Bound Done
  seek target = fromSorted (dropWhile (not . satisfies target . fst) l)
\end{minted}

\noindent
To define intersection we need one last helper, which extracts a lower bound on the remaining keys in an iterator:

\begin{minted}{haskell}
bound :: Seek k v -> Bound k
bound (Seek (Found k _) _) = Atleast k
bound (Seek (Bound p)   _) = p
\end{minted}

\noindent
Finally, we can define fair intersection of seekable iterators.
If both iterators are at the same key and have found values, we've found an element of the intersection; otherwise we only know there are no keys until after the greater of their bounds.
To seek, we simply seek our sub-iterators.
\todo{Point out that this \emph{bounds} the work done by a single call to seek.}

\begin{minted}{haskell}
intersect :: Ord k => Seek k a -> Seek k b -> Seek k (a,b)
intersect s t = Seek posn' seek' where
  posn' | Found k x <- posn s, Found k' y <- posn t, k == k' = Found k (x, y)
        | otherwise = Bound (bound s `max` bound t)
  seek' k = seek s k `intersect` seek t k
\end{minted}

\noindent
The natural next step is to try our evens-odds-ends example.
Unfortunately, we now run into the ``white lie'' noted in footnote 5 in \cref{fn:white-lie}:
we cannot expect an asymptotic speedup while we are still using linear-access Haskell lists; the actual effects we saw there were due to interpretation overhead.
In a small experiment with Haskell's Arrays package, compiled with \ttt{ghc -O}, on arrays of 30 million elements, our na\"ive \hask{Iter} approach from \cref{sec:unfair-intersection} took 1.2 seconds to enumerate \ttt{(evens `intersect` odds) `intersect` evens}, while using our \hask{Seek} iterators (or reassociating) took less than a millisecond.
You can find the benchmarking code for \hask{Iter} in \cref{fig:iter-benchmark}; the code for \hask{Seek} is nearly the same, except for an altered \ttt{fromSortedArray} method, shown in \cref{fig:seek-fromsortedarray}.

%% %% TODO FIXME: FINISH THIS
%% \todo{TODO: ok now what do I say? Oh, obviously I test this on my previous example: evens, odds, ends.
%%   UNFORTUNATELY, this (a) runs straight into the white lie I told earlier---it's super slow unless we compile it; (b) when we compile, the three-way intersection is indeed faster than (intersect evens odds), which is good, but (c) why should it be? it's the same asymptotic complexity! it appears that dropWhile is ~3.5--4$\times$ faster than ping-ponging between evens \& odds, which is... a larger constant factor than I expected, but okay.
%% }


%% \begin{figure}
%%   \begin{minted}[xleftmargin=0em, linenos]{haskell}
%% data Bound k
%%   = Init             -- anything satisfies Init
%%   | Atleast k        -- is it >= k?
%%   | Greater k        -- is it  > k?
%%   | Done             -- nothing satisfies Done
%%   deriving Eq

%% data Position k v
%%   = Found k v        -- here's a key and its value
%%   | Bound (Bound k)  -- the next key satisfies this bound

%% data Seek k v = Seek
%%   { posn :: Position k v          -- a key-value pair, or a bound
%%   , seek :: Bound k -> Seek k v } -- seeks forward toward a bound

%% -- Init < ... < Atleast n < Greater n < Atleast (n+1) < ... < Done
%% instance Ord k => Ord (Bound k) where
%%   compare x y = embed x `compare` embed y
%%     where embed Init        = (0, Nothing)
%%           embed (Atleast k) = (1, Just (k, 0))
%%           embed (Greater k) = (1, Just (k, 1))
%%           embed Done        = (2, Nothing)

%% bound :: Seek k v -> Bound k
%% bound (Seek (Found k v) _) = Atleast k
%% bound (Seek (Bound p)   _) = p

%% fromSorted :: Ord k => [(k, v)] -> Seek k v
%% fromSorted l = Seek posn seek where
%%   posn = case l of [] -> Bound Done; (k,v):_ -> Found k v
%%   seek target = fromSorted $ dropWhile ((target >) . Atleast . fst) l

%% toSorted :: Seek k v -> [(k, v)]
%% toSorted (Seek (Bound Done) _)    = []
%% toSorted (Seek (Found k v)  seek) = (k, v) : toSorted (seek (Greater k))
%% toSorted (Seek (Bound k)    seek) = toSorted (seek k)

%% intersect :: Ord k => Seek k a -> Seek k b -> Seek k (a,b)
%% intersect s t = Seek posn' seek' where
%%   posn' | Found k x <- posn s, Found k' y <- posn t, k == k' = Found k (x, y)
%%         | otherwise = Bound (bound s `max` bound t)
%%   seek' k = intersect s' t' where
%%     s' = seek1 k
%%     t' = seek2 (bound s') -- leapfrog optimization; could be (seek2 k) instead
%%   \end{minted}
%%   \caption{Fair intersection of seekable iterators}
%%   \label{fig:fair-iterators}
%% \end{figure}


%% \section{sketches}
%% \todo{TODO SKETCHES}

%% miniKanren's key idea over Prolog is to have a complete search strategy by using a FAIR strategy for disjunctions.
%% We accomplish this by BOUNDING the time we spend in each disjunct before switching to the other.

%% FAIR: no branch gets ``starved'' by another branch; with enough time, we investigate all branches arbitrarily far.

%% BOUNDED: we try a branch for only a bounded amount of time, so that we don't get STUCK. This is the purpose of the ``thunk'' constructor for streams (what's the standard name for this in mK?).

%% Two uses of fairness:

%% 1. Implementing $\lambda_{\vee}$.
%%    semantics are nondeterministic,
%%    so SEARCH!

%%    This search is inefficient because it fails to take into account the *monotonic structure* of evaluation in lambda join: we are in essence branching on *how precise* to make our evaluation.
%%    The branching is not between mutually exclusive alternatives, where each contributes information the other lacks, but where one alternative has strictly more information than another.

%%    (how does this actually result in combinatorial explosion, though?)

%% 2. Work in progress on a seekable iterator interface for compositionally worst-case optimal joins.
%%    The key idea is to FAIRLY incorporate information from all iterators contributing to the join,
%%    which we do by BOUNDING how much work we put into one iterator before moving to the next.

%%    This implements FAIR CONJUNCTION, and it's FAST.

%%    But it takes advantage of having a comprehensive view of data, so we can scan through it in sorted order - we can't define these things recursively, therefore it can't be turing-complete.
%% if we decide to use ``feedback with delay'' to overcome this, we have perhaps reinvented, not miniKanren, but Datalog!


\begin{figure}
  \begin{minted}[fontsize=\footnotesize]{haskell}
import Data.Array
import Text.Printf
import System.CPUTime
import System.IO (hFlush, stdout)

n = 30_000_000
evens = fromSortedArray [(x, "even") | x <- [0, 2 .. n]]
odds  = fromSortedArray [(x, "odd")  | x <- [1, 3 .. n]]
ends  = fromSortedArray [(x, "end")  | x <- [0,      n]]

fromSortedArray :: Ord k => [(k,v)] -> Iter k v
fromSortedArray l = go 0 where
  arr = listArray (0, hi) l
  hi = length l
  go lo = if lo >= hi then Empty else Yield k v seek where
    (k, v) = arr ! lo
    seek tgt = go $ gallop ((tgt <=) . fst . (arr !)) (lo + 1) hi

-- galloping search: exponential probing followed by binary search.
-- O(log i) where i is the returned index.
gallop :: (Int -> Bool) -> Int -> Int -> Int
gallop p lo hi | lo >= hi  = hi
               | p lo      = lo
               | otherwise = go lo 1 where
  go lo step | lo + step >= hi = bisect lo hi
             | p (lo + step)   = bisect lo (lo + step)
             | otherwise       = go (lo + step) (step * 2)
  bisect l r | r - l <= 1 = r
             | p mid      = bisect l mid
             | otherwise  = bisect mid r
    where mid = (l+r) `div` 2

printTime :: String -> a -> IO ()
printTime label result = do
  putStr (label ++ ": "); hFlush stdout
  start_ps <- getCPUTime --in picoseconds
  end_ps <- result `seq` getCPUTime
  printf "%0.6fs\n" (fromIntegral (end_ps - start_ps) / (10^12))

{-# NOINLINE time2 #-}
time2 label xs ys = printTime label $ length $ toSorted $ xs `intersect` ys
{-# NOINLINE time3L #-}
time3L label xs ys zs = printTime label $ length $ toSorted $ (xs `intersect` ys) `intersect` zs
{-# NOINLINE time3R #-}
time3R label xs ys zs = printTime label $ length $ toSorted $ xs `intersect` (ys `intersect` zs)

thrice x = do x; x; x
main = do thrice $ time2 "odds & ends" odds ends
          thrice $ time2 "evens & odds" evens odds
          thrice $ time3L "(evens & odds) & ends" evens odds ends
          thrice $ time3R "evens & (odds & ends)" evens odds ends
  \end{minted}
  \caption{Benchmarking intersection for \hask{Iter}}
  \label{fig:iter-benchmark}
\end{figure}

\begin{figure}
  \begin{minted}{haskell}
fromSortedArray :: Ord k => [(k,v)] -> Seek k v
fromSortedArray l = go 0 where
  hi = length l
  arr = listArray (0, hi) l
  go lo = Seek pos sek where
    pos | lo >= hi  = Bound Done
        | otherwise = Found k v where (k,v) = arr ! lo
    sek tgt = go $ gallop (satisfies tgt . fst . (arr !)) lo hi
  \end{minted}
  \caption{fromSortedArray for \hask{Seek}}
  \label{fig:seek-fromsortedarray}
\end{figure}


\section{Related and future work}

Intersection is a special case of relational join, and the technique described here extends to full conjunctive queries via \emph{nested} intersections.
This is the `trie' in Leapfrog Triejoin: for instance, a finite ternary relation $R \subseteq A \times B \times C$ can be represented as a 3-level trie, i.e.\ a nested seekable iterator \hask{Seek a (Seek b (Seek c ()))}.
By intersecting on each join column successively, we implement a worst-case optimal join.
Worst-case optimal joins therefore seem to be a form of \emph{fair conjunction,} a connection which we are not sure has been noticed previously.

These nested seekable iterators also support disjunction/union, similar to the merge step in mergesort.
We believe it is possible to extend this to existential quantification, essentially a disjunction over an entire trie level, using a priority queue of seekable iterators sorted by position.
Moreover, by changing the lowest level of the trie from the unit type \ttt{()} to some other type \ttt{v}, e.g.\ \hask{Seek a (Seek b (Seek c v))}, nested seekable iterators support \emph{weighted logic programming}, where tuples of a relation have an associated value.
Conventionally these values are drawn from a semiring: conjunction multiplies, while disjunction and existentials sum.

Together these amount to an implementation of tensor algebra.
Indeed, our \hask{Seek} interface is effectively a variant of the \emph{indexed streams}~\citet{indexed-streams} use as a tensor algebra compiler intermediate representation.
Indexed streams replace the \hask{Position} of an iterator with three values: its \emph{index} (\emph{key} in our terminology), a \emph{ready flag} indicating whether it has found a value, and its \emph{value} (which should only be accessed when the ready flag is set).
In place of \hask{seek} they have a \emph{skip} function which takes a key and a flag indicating whether to use \hask{Atleast} or \hask{Greater} semantics.
\todo{all their key/index types are bounded so they don't need \hask{Done}---is this accurate?}

To extend to full weighted logic programming, however, we need \emph{recursion,} which is hard to reconcile with the sorted nature of seekable iterators: self-reference means the existence of a larger key might imply the existence of a smaller one, so we may discover a key exists when it's too late to produce it.
One approach might be to introduce an artifical time dimension, e.g.\ to represent a relation by a sequence of seekable iterators, representing a monotonically growing set, with recursion implemented as feedback with delay.
This resembles a na\"ive implementation of Datalog; perhaps semina\"ive evaluation is also possible.


%% ---------- BIBLIOGRAPHY ----------
\bibliography{on-fairness}

\end{document}
